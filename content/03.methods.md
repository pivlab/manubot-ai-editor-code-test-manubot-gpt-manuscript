## Implementing AI-based revision into the Manubot publishing ecosystem

We propose a human-centric methodology for integrating Artificial Intelligence (AI) into the process of academic manuscript writing.
This methodology is structured around a collaborative framework between human authors and AI, specifically large language models (LLMs).
The process unfolds in several distinct steps: 

1) Human authors initially draft the manuscript content, establishing the foundational narrative and scientific arguments.
2) Subsequently, an LLM reviews this initial manuscript draft, generating a comprehensive set of suggested revisions.
These suggestions can range from grammatical corrections to more substantive content recommendations.
3) Human authors then meticulously review these suggested changes.
Only the edits that are deemed appropriate and accurate by the authors are integrated into the manuscript.

This approach is designed to leverage the strengths of both human expertise and AI capabilities, with a particular emphasis on human oversight to prevent the introduction of inaccuracies or misleading information into the academic discourse.
To operationalize this human-centric approach, we developed a specialized tool known as the Manubot AI Editor.
This tool is a novel component of the Manubot framework, which is specifically designed to support scholarly publishing.
The Manubot infrastructure facilitates a seamless integration of AI-assisted authoring within the broader context of academic writing and publishing [@doi:10.1371/journal.pcbi.1007128].

### Overview of the Manubot AI Editor

![
**AI-based revision applied on a Manubot-based manuscript.**
**a)** A manuscript (written with Manubot) with different sections.
**b)** The prompt generator integrates metadata using prompt templates to generate section-specific prompts for each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections' prompts include these instructions: *"the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure"*, although these are only shown for abstracts.
Our tool allows the user to provide a custom prompt instead of using the default ones shown here.
](images/figure_1.svg "AI-based revision applied on a Manubot manuscript"){#fig:ai_revision width="75%"}

The Manubot AI Editor represents a pioneering infrastructure for AI-assisted revision, built atop the Manubot platform, which is renowned for facilitating the collaborative writing of scientific manuscripts through integration with version control systems like GitHub [@doi:10.1371/journal.pcbi.1007128].
This integration not only simplifies the tracking of modifications and collaboration in real-time but also streamlines the production of formatted manuscripts in various formats such as HTML, PDF, and DOCX.
An illustration of the HTML formatted output is provided in Figure {@fig:ai_revision}a.
Leveraging the principles of openness and modernity, the Manubot AI Editor, accessible at [https://github.com/manubot/manubot-ai-editor](https://github.com/manubot/manubot-ai-editor), is composed of three main components: 

1) A Python library, offering a suite of classes and functions for reading the manuscript's content and metadata, invoking Large Language Models (LLMs) for text revision, and subsequently recording the outcomes.
2) A GitHub Actions workflow, which employs the aforementioned Python library within GitHub's ecosystem to maintain a transparent record of the provenance information.
3) A prompt generator that utilizes the manuscript's metadata in conjunction with predefined templates to craft section-specific prompts for each paragraph, as depicted in Figure {@fig:ai_revision}b.

This comprehensive approach ensures that the Manubot AI Editor not only enhances the efficiency and quality of academic writing but also upholds the principles of transparency and collaboration that are fundamental to the scientific community.


The GitHub Actions workflow we developed facilitates a seamless process for users to initiate an automated revision task, which can be applied to the manuscript as a whole or to selected sections.
Upon activation of this workflow, the manuscript undergoes a parsing process that organizes the content by section and subsequently by paragraph, as illustrated in Figure {@fig:ai_revision}b.
This structured content is then fed into the language model alongside a series of predefined prompts.
In response, the model generates a revised version of the text.
Utilizing the GitHub API, our workflow automates the creation of a new pull request.
This step provides users with the opportunity to scrutinize and adjust the suggested modifications prior to their integration into the manuscript.
A critical feature of our workflow is its ability to distinguish between contributions made by human authors and those generated by the AI language model.
This distinction is particularly relevant in anticipation of potential future legal rulings that may redefine the copyright status of content produced by generative models.


In our research, we utilized the OpenAI API for accessing advanced language models, as this platform provides a comprehensive suite of models suitable for our purposes (OpenAI, n.d.).
Recognizing that the usage of this API incurs variable costs based on the length of the manuscript, we developed a specialized workflow within GitHub Actions.
This workflow is designed to be manually activated by the user, offering a flexible approach to managing operational costs.
Specifically, our system permits users to selectively choose segments of their manuscript for revision, rather than obligating the revision of the entire document.
This feature is particularly useful for tailoring expenses according to individual needs.

Moreover, our framework provides the capability to adjust various model parameters, thereby offering users further control over the cost-effectiveness of the process.
These parameters include the selection of the language model version, which encompasses options such as Davinci and Curie, as well as the more recent GPT-3.5 Turbo and GPT-4 models.
The choice of model directly influences the cost and quality of the output, allowing users to make decisions based on their specific requirements and budget constraints.
Additionally, users can modify settings related to the model's risk tolerance and the desired "quality" of the completions, enabling a more customized experience.

For example, employing the Davinci models typically incurs a cost of less than $0.50 per manuscript run, demonstrating the cost-efficiency of our approach for most academic manuscripts.
This cost structure ensures that our method remains accessible to a wide range of users, from individual researchers to larger institutions, seeking to leverage the capabilities of AI-assisted academic authoring.


### Implementation details

To initiate the workflow, the user is required to designate the branch targeted for revision, choose specific files or sections of the manuscript for selective editing (this step is optional), determine the language model to be utilized (with `text-davinci-003` set as the default option), input an optional custom prompt (although section-specific prompts are employed by default), and specify the name of the output branch.
For users with more technical expertise, the system offers the flexibility to modify the majority of the tool's operational parameters or the settings of the language model.


When the workflow is initiated, it proceeds to download the manuscript by cloning the designated branch.
This involves revising either all manuscript files or a specified subset if delineated by the user.
Subsequently, the system reads each paragraph within the file and submits it to the OpenAI API for revision.
Upon a successful request, the tool replaces the original paragraph with the revised version, adhering to the format of one sentence per line, as this layout is recommended for the input text.
In instances where the request encounters failure, the tool may attempt resubmission (up to a maximum of five retries by default) if the error is recognized as common (e.g., "server overloaded") or is specific to the model, necessitating an adjustment in its parameters.
Should the error prove unmanageable or if the retry limit is reached, the tool preserves the original paragraph but precedes it with an HTML comment that elucidates the error's nature.
This feature facilitates user intervention, allowing for problem diagnosis and potential resolution.


As demonstrated in Figure {@fig:ai_revision}b, every API request is composed of two main elements: a prompt (the instructions provided to the model) and the paragraph that requires revision.
In the absence of a user-defined custom prompt, the system employs a section-specific prompt generator that leverages the manuscript's title and keywords to ensure accuracy and optimize revision outcomes.
It is imperative that both the title and keywords are precisely defined to achieve the most effective revisions.
Another critical factor in processing a paragraph is its sectional context.
For example, an abstract comprises sentences that typically lack citations, while a paragraph from the Introduction section may contain numerous references to other scholarly works.
A paragraph situated in the Results section might include fewer citations but should have ample references to figures or tables, providing sufficient detail about the experiments to facilitate understanding and interpretation of the findings.
The Methods section's requirements can vary depending on the nature of the paper; however, it generally necessitates the inclusion of technical descriptions and, on occasion, mathematical formulas and equations.
To address these nuanced needs, we have developed section-specific prompts, which have proven to yield the most pertinent suggestions.
It is important to note that figure and table captions, as well as paragraphs comprising only one or two sentences and containing fewer than sixty words, are not subjected to processing and are instead directly transferred to the output document without modification.


The methodology of our study is delineated with a focus on the development and deployment of a publishing infrastructure that integrates artificial intelligence (AI) for academic authoring.
This infrastructure leverages the Manubot platform, a software tool that facilitates collaborative writing and publishing of scientific manuscripts on GitHub (Himmelstein et al., 2019).
The integration of AI, particularly large language models (LLMs), aims to enhance the authoring process by providing suggestions for text improvement, citation recommendations, and assistance in data analysis.

Our approach to incorporating AI into the Manubot platform involved the development of a plugin that interfaces with several pre-trained LLMs.
The selection of LLMs was based on their performance in natural language processing tasks as reported in recent literature (Devlin et al., 2018; Brown et al., 2020).
To facilitate the interaction between Manubot and the LLMs, we implemented a RESTful API that allows for seamless communication and data exchange.

The process of integrating AI into the academic authoring workflow is governed by a set of algorithms that determine the most appropriate moments for the AI to offer suggestions.
This decision-making process is modeled by the following equation:

$$
S_i = f(C_i, T_i, P_i)
$$

where \(S_i\) represents the suggestion provided by the AI for the \(i\)-th instance, \(C_i\) denotes the current context of the manuscript at the \(i\)-th instance, \(T_i\) signifies the type of assistance requested (e.g., text improvement, citation recommendation), and \(P_i\) represents the priority of the request.
The function \(f\) encapsulates the logic used to evaluate the appropriateness of an AI intervention


### Properties of language models

<!-- We mainly focused on the completion endpoint, as the edits endpoint was currently in beta. -->
The Manubot AI Editor incorporates the [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) to facilitate the processing of individual paragraphs.
Our evaluation of the tool's effectiveness involved employing both the Davinci and Curie models from the GPT-3 series, specifically the `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001` iterations.
The Davinci models, within the GPT-3 suite, are recognized for their superior capabilities, in contrast, the Curie models, though not as advanced, offer advantages in terms of speed and cost-efficiency.
The capability exists to fine-tune all models by manipulating various parameters, a process detailed in the [OpenAI - API Reference](https://platform.openai.com/docs/api-reference/chat/create).
This fine-tuning process is crucial for optimizing the performance of the AI models in specific contexts, and our tool simplifies the adjustment of these critical parameters for users.


<!-- Therefore, it is not possible to use the entire manuscript as input, not even entire sections. -->
Language models designed for text completion possess a defined context length, which represents the maximum number of tokens they can interpret in a single instance.
Tokens are essentially sequences of characters frequently occurring within the text.
This context length encompasses the dimensions of both the initial prompt provided to the model and the paragraph in question, in addition to the upper limit on the number of tokens designated for generation in the completion process, identified by the parameter `max_tokens`.
For example, the context length for Davinci models stands at 4,000, whereas for Curie models, it is set at 2,048 [OpenAI - Models overview](https://platform.openai.com/docs/models/gpt-3).
To circumvent surpassing this context length limitation, our AI-assisted revision tool processes individual paragraphs of the manuscript utilizing section-specific prompts, as depicted in Figure {@fig:ai_revision}b.
This methodology facilitates the handling of extensive manuscripts by segregating them into more manageable text segments.
Nevertheless, this segmentation implies that the language model is restricted to processing a sole paragraph from each section at a time, potentially leading to a loss of critical context necessary for generating enhanced outputs.
Despite this limitation, our findings indicate that the model consistently delivers revisions of superior quality [Results](#sec:results).
Moreover, the `max_tokens` parameter is determined by doubling the anticipated token count within the paragraph, with the understanding that one token is roughly equivalent to four characters [OpenAI - Tokenizer](https://platform.openai.com/tokenizer).
Should the API return an error related to this parameter, the tool is programmed to automatically modify the `max_tokens` parameter and reattempt the request.
Users also have the discretion to either maintain a constant value for `max_tokens` across all paragraphs or to adjust the maximum token fraction in relation to the estimated size of the paragraph, with the default multiplier being two.


The language models we employed exhibit stochastic behavior, which implies that for the same input, they are capable of generating varied revisions each time.
This variability can be finely tuned through the adjustment of parameters such as the "sampling temperature" or "nucleus sampling," with a default setting of `temperature=0.5`.
Although these default parameters have been empirically determined to yield satisfactory outcomes across a diverse range of manuscripts, they are not fixed and can be modified to render the model's output more predictable.
Furthermore, users have the option to command the model to produce multiple outputs and then select the one that demonstrates the highest log probability per token, a strategy that can enhance the revision's quality.
In our system, we generate a single completion by setting `best_of=1` to keep the process cost-effective for the user.
Our approach also introduces flexibility in manuscript processing, allowing users to either process the entire document or focus on specific sections.
This feature is particularly beneficial for cost-effective management, enabling users to iterate over a single text segment multiple times until the most satisfactory revision is achieved.


### Installation and use

The Manubot AI Editor, an integral part of the Manubot platform's standard template manuscript—commonly referred to as rootstock—is accessible via [https://github.com/manubot/rootstock](https://github.com/manubot/rootstock).
Individuals interested in utilizing this workflow are required to adhere to the established Manubot installation procedures.
Detailed instructions for activating the AI-assisted authoring tool are provided in the `USAGE.md` file located within the rootstock repository.
Upon successful activation, the workflow, designated as `ai-revision`, becomes readily available for use and can be accessed through the Actions tab within the user's manuscript repository.
This streamlined integration facilitates a seamless transition to AI-assisted academic authoring, underscoring the utility and innovative capabilities of the Manubot platform in enhancing scholarly publishing processes.
