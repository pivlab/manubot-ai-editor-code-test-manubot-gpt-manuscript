## Implementing AI-based revision into the Manubot publishing ecosystem

We propose a human-centric approach for the use of artificial intelligence in manuscript writing, which consists of the following steps:
1) Human authors write the manuscript content.
2) A large language model (LLM) revises the manuscript, generating a set of suggested changes.
3) Human authors review the suggested changes, and the approved edits are then integrated into the manuscript.

By focusing on human review, this approach attempts to mitigate the risk of generating incorrect or misleading information.
To implement this human-centric approach, we developed a tool called the Manubot AI Editor, which is part of the Manubot infrastructure for scholarly publishing [@doi:10.1371/journal.pcbi.1007128].

$$ 
LLM: Large Language Model
$$

### Overview of the Manubot AI Editor

![
**AI-based revision applied on a Manubot-based manuscript.**
**a)** A manuscript (written with Manubot) with different sections.
**b)** The prompt generator integrates metadata using prompt templates to generate section-specific prompts for each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections' prompts include these instructions: *"the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure"*, although these are only shown for abstracts.
Our tool allows the user to provide a custom prompt instead of using the default ones shown here.
](images/figure_1.svg "AI-based revision applied on a Manubot manuscript"){#fig:ai_revision width="75%"}

The Manubot AI Editor is an AI-based revision infrastructure incorporated into Manubot [@doi:10.1371/journal.pcbi.1007128], a tool designed for collaborative scientific manuscript writing.
Manubot seamlessly interfaces with popular version control platforms like GitHub, enabling authors to effortlessly monitor changes and work together on manuscripts in real-time.
Additionally, Manubot streamlines the manuscript formatting process, producing outputs in various formats (e.g., HTML, PDF, DOCX; Figure {@fig:ai_revision}a illustrates the HTML output).
Rooted in a contemporary and open framework, our Manubot AI Editor ([https://github.com/manubot/manubot-ai-editor](https://github.com/manubot/manubot-ai-editor)) comprises three key components: 1) a Python library that furnishes classes and functions for reading manuscript content and metadata, invoking the Large Language Model (LLM) for automated text revisions, and saving the outcomes; 2) a GitHub Actions workflow that employs our Python library within GitHub to uphold provenance details for transparency; 3) a prompt generator that merges the manuscript's metadata into prompt templates to formulate section-specific prompts for each paragraph (Figure {@fig:ai_revision}b). 

$$
\text{Symbols:} \\
\text{LLM} - \text{Large Language Model}
$$


The GitHub Actions workflow enables users to easily initiate an automated revision task on the entire manuscript or specific sections of it.
When the action is triggered, the manuscript is parsed by section and then by paragraph (Figure (@fig:ai_revision)b), which are passed to the language model along with a set of custom prompts.
The model then provides a revised version of the text.
Our workflow utilizes the GitHub API to create a new pull request, allowing the user to review and modify the output before merging the changes into the manuscript.
This workflow attributes text to either the human user or the AI language model, which may be crucial in light of potential future legal decisions that could impact the copyright landscape surrounding the outputs of generative models.

$$ ...
$$ {#id}

In this equation, the symbols are defined as follows:
- Symbol1: Definition of Symbol1.
- Symbol2: Definition of Symbol2.


We utilized the [OpenAI API](https://openai.com/api/) to access these models.
Since this API incurs a cost with each run that depends on manuscript length, we developed a workflow in GitHub Actions that can be manually triggered by the user.
Our implementation allows users to adjust costs to their needs by selecting specific sections for revision instead of the entire manuscript.
Additionally, various model parameters can be modified to further adjust costs, including the language model version (such as Davinci and Curie, the current GPT-3.5 Turbo and GPT-4, and potentially newly published ones), the risk level the model will take, or the "quality" of the completions.
For example, when using Davinci models, the cost per run is typically under $0.50 for most manuscripts.

$$
\text{Symbols:} \\
\text{API} - \text{Application Programming Interface}
$$


### Implementation details

To run the workflow, the user must specify the branch that will be revised, select the files or sections of the manuscript (optional), specify the language model to use (default is `text-davinci-003`), provide an optional custom prompt (default is section-specific prompts), and specify the output branch name.
Advanced users can also modify most of the tool's behavior or the language model parameters. 

$$
\text{Symbols:}\\
\text{branch} - \text{selected branch for revision}\\
\text{files/sections} - \text{files or sections of the manuscript to be revised (optional)}\\
\text{language model} - \text{model used for text generation}\\
\text{custom prompt} - \text{user-defined prompt for the AI model}\\
\text{output branch} - \text{name of the branch where revised content will be stored}
$$


When the workflow is triggered, it downloads the manuscript by cloning the specified branch.
It revises all of the manuscript files, or only some of them if the user specifies a subset. 

Next, each paragraph in the file is read and submitted to the OpenAI API for revision.
If the request is successful, the tool will write the revised paragraph in place of the original one, using one sentence per line (which is the recommended format for the input text).
If the request fails, the tool might try again (up to five times by default) if it is a common error (such as "server overloaded") or a model-specific error that requires changing some of its parameters.
If the error cannot be handled or the maximum number of retries is reached, the original paragraph is written instead with an HTML comment at the top explaining the cause of the error. 

This allows the user to debug the problem and attempt to fix it if desired.


As shown in Figure (@fig:ai_revision) b, each API request comprises a prompt (the instructions given to the model) and the paragraph to be revised.
Unless the user specifies a custom prompt, the tool will use a section-specific prompt generator that incorporates the manuscript title and keywords.
Therefore, both must be accurate to obtain the best revision outcomes.

The other key component to process a paragraph is its section.
For instance, the abstract is a set of sentences with no citations, whereas a paragraph from the Introduction section has several references to other scientific papers.
A paragraph in the Results section has fewer citations but many references to figures or tables, and must provide enough details about the experiments to understand and interpret the outcomes.
The Methods section is more dependent on the type of paper, but in general, it has to provide technical details and sometimes mathematical formulas and equations.

Therefore, we designed section-specific prompts, which we found led to the most useful suggestions.
Figure and table captions, as well as paragraphs that contain only one or two sentences and fewer than sixty words, are not processed and are copied directly to the output file.

$$
\text{Symbols:}
$$

- API: Application Programming Interface


The Manubot platform is a novel publishing infrastructure that leverages artificial intelligence to assist academic authors in creating scholarly content.
By integrating large language models into the authoring process, Manubot aims to streamline the creation of scientific manuscripts and enhance the overall quality of scholarly publishing.
This platform enables authors to collaborate on writing projects, manage citations, and automatically generate references in various citation styles.
Additionally, Manubot provides tools for version control, enabling authors to track changes and revisions throughout the writing process.
The use of artificial intelligence in academic authoring has the potential to revolutionize the way research is disseminated and accessed by the scientific community. 

$$
\text{Symbols:} \\
\text{Manubot} - \text{Manubot platform} \\
\text{AI} - \text{artificial intelligence}
$$


### Properties of language models

<!-- We mainly focused on the completion endpoint, as the edits endpoint was currently in beta. -->
The Manubot AI Editor utilizes the Chat Completions API to process each paragraph.
Our tool has been tested with both the Davinci and Curie models, specifically including text-davinci-003, text-davinci-edit-001, and text-curie-001.
Among the GPT-3 models, the Davinci models are the most powerful, while the Curie models are less capable but faster and more cost-effective.
All models are capable of being fine-tuned with different parameters (refer to OpenAI - API Reference), and our tool allows for easy adjustment of the most important parameters. 

Equation ($$ ...
$$ {#id}) definitions:

$$ GPT-3 = Generative Pre-trained Transformer 3 $$
$$ API = Application Programming Interface $$


<!-- Therefore, it is not possible to use the entire manuscript as input, not even entire sections. -->
Language models for text completion have a context length that indicates the limit of tokens they can process (tokens are common character sequences in text).
This limit includes the size of the prompt and the paragraph, as well as the maximum number of tokens to generate for the completion (parameter `max_tokens`).
For instance, the context length of Davinci models is 4,000 and for Curie, it is 2,048 (see [OpenAI - Models overview](https://platform.openai.com/docs/models/gpt-3)).

$$
\text{Context length} = \text{size of prompt} + \text{paragraph size} + \text{max_tokens}
$$

To ensure we never exceed this context length, our AI-assisted revision software processes each paragraph of the manuscript with section-specific prompts, as shown in Figure {@fig:ai_revision}b.
This approach allows us to process large manuscripts by breaking them into smaller chunks of text.
However, since the language model only processes a single paragraph from a section, it can potentially lose important context needed to produce a better output.
Nonetheless, we find that the model still produces high-quality revisions (see [Results](#sec:results)).

$$
\text{Paragraph size} \approx \text{number of tokens} \times 4 \text{ characters}
$$

Additionally, the maximum number of tokens (parameter `max_tokens`) is set as twice the estimated number of tokens in the paragraph.
The tool automatically adjusts this parameter and performs the request again if a related error is returned by the API.
The user can also force the tool to either use a fixed value for `max_tokens` for all paragraphs or change the fraction of maximum tokens based on the estimated paragraph size (two by default).


The language models used in this study are stochastic, meaning they generate a different revision for the same input paragraph each time.
This behavior can be adjusted by using the "sampling temperature" or "nucleus sampling" parameters (we use `temperature=0.5` by default).
Although we selected default values that work well across multiple manuscripts, these parameters can be changed to make the model more deterministic.
The user can also instruct the model to generate several completions and select the one with the highest log probability per token, which can improve the quality of the revision.
Our implementation generates only one completion (parameter `best_of=1`) to avoid potentially high costs for the user.

$$
\text{Symbol Definitions:}\\
\text{Stochastic Language Models (SLM)} \\
\text{Sampling Temperature (ST)} \\
\text{Nucleus Sampling (NS)} \\
\text{Log Probability per Token (LPT)} \\
\text{Best of (Bo)}
$$

Additionally, our workflow allows the user to process either the entire manuscript or individual sections.
This provides more cost-effective control while focusing on a single piece of text, wherein the user can run the tool several times and pick the preferred revised text.


### Installation and use

The Manubot AI Editor is part of the standard Manubot template manuscript, referred to as rootstock, and is available at [https://github.com/manubot/rootstock](https://github.com/manubot/rootstock).
Users wishing to use the workflow only need to follow the standard procedures to install Manubot.
The section "AI-assisted authoring", found in the file `USAGE.md` of the rootstock repository, explains how to enable the tool.
Afterward, the workflow (named `ai-revision`) will be available and ready to use under the Actions tab of the user's manuscript repository.

$$
\text{Symbols:} \\
\text{AI} - \text{Artificial Intelligence}
$$
