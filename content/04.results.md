## Evaluations of AI-based revisions {#sec:results}

### Evaluation setup

<!-- In addition, there could be multiple different yet valid revisions with a varying degree of quality. -->
Evaluating the effectiveness of text generation, particularly for revising scientific content, poses significant challenges.
It is crucial to ensure that revisions do not alter the original intent or introduce inaccuracies.
To address these concerns, our study prioritized human evaluations of the content revisions.
We applied this methodology to assess our tool's performance, using three manuscripts authored by our team.
This approach enabled us to objectively determine whether the revisions preserved the original meaning and essential details.
Additionally, during the development of our prompts—a process known as prompt engineering—we employed a unit testing framework.
This step was critical to guarantee that the output from our prompts adhered to specific quality standards.

#### Language models

We assessed our AI-assisted revision workflow with three GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two models, `text-davinci-003` and `text-davinci-edit-001`, are among the most advanced GPT-3 Davinci models, with the former being a production-ready model for generating text and the latter, still in beta during our tests, designed specifically for editing text by accepting both instructions and the text to be revised.
The `text-curie-001` model, while faster and more cost-effective, is also considered highly capable.
These distinctions are important for understanding the models' applications in streamlining the manuscript revision process.

#### Manuscripts

| Manuscript ID  | GitHub URL  | Title | Keywords |
|:-----|:-------|:--------------|:------|
| CCC | [greenelab/ccc-manuscript](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning | correlation coefficient, nonlinear relationships, gene expression |
| PhenoPLIER | [greenelab/phenoplier_manuscript](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| Manubot-AI | [greenelab/manubot-gpt-manuscript](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them. {#tbl:manuscripts}


<!-- Using these manuscripts, we tested and improved our prompts. -->
<!-- Our findings are reported below. -->

#### Evaluation using human assessments

We enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC, PhenoPLIER, and Manubot-AI).
This added the "ai-revision" workflow to the "Actions" tab of each repository.
We triggered the workflow manually and used the three language models described above to produce one pull request (PR) per manuscript and model.
These PRs can be accessed from the "Pull requests" tab of each repository.
They are titled *"GPT (MODEL) used to revise manuscript"*, with *MODEL* being the identifier of the model used.
The PRs show all the differences between the original text and the AI-based revision suggestions.
<!-- We discuss below our findings based on these PRs across different sections of the manuscripts. -->
We evaluated our tool with three manuscripts we authored, as listed in Table @tbl:manuscripts: the Clustermatch Correlation Coefficient (CCC), PhenoPLIER, and Manubot-AI (this manuscript).
The CCC study introduces a novel correlation coefficient for analyzing transcriptomic data.
In contrast, PhenoPLIER presents a comprehensive framework that combines three methods for genetic research.
Specifically, CCC focuses on a single computational method for correlating gene expression data, making it a straightforward example of computational biology.
PhenoPLIER, however, spans genomic medicine and integrates various approaches—including regression, clustering, and drug-disease prediction—by utilizing a wide range of data sources such as genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and responses to small molecule perturbations.
This makes PhenoPLIER a more complex manuscript, featuring a detailed Methods section with equations, and a higher count of figures and tables.
Lastly, the Manubot-AI manuscript, which we used to demonstrate a practical application of our AI-assisted authoring tool, has a simpler structure.
It was both written and revised with the assistance of our tool before submission, showcasing its effectiveness in streamlining the academic writing process.


When manually assessing the quality of the revisions, we considered whether the revision: 1) preserve the original meaning, 2) preserve important details, 4) introduced new and incorrect information, and 5) preserve the correct Markdown format (e.g., citations, equations).


#### Prompt engineering

In our study, we rigorously evaluated our tool through comprehensive unit testing, focusing on its ability to process and enhance academic manuscripts with the aid of artificial intelligence.
Our tests encompassed various aspects of manuscript processing, including paragraph segmentation, generating tailored prompts based on manuscript metadata, and integrating text suggestions while preserving the original writing style.
Crucially, we assessed the quality of text revisions, ensuring that the output met specific quality standards.

For example, we verified that revised abstracts were concise, starting with a capital letter, ending with a period, and excluding citations.
In the introduction, we ensured a significant portion of citations were retained, giving the model leeway to omit unnecessary text.
Our findings indicated that a simple instruction to maintain most citations was effective with our most advanced model.
Additionally, we confirmed that this model correctly formatted citations and did not require prompt adjustments to recognize the Manubot/Markdown citation format (e.g., `[@doi:...]`).

Our evaluation also included tests for handling inline LaTeX formulas and references to figures, tables, and equations, where the advanced model accurately maintained the correct format in most instances.
For the Methods section, beyond the tests mentioned, we assessed the model's proficiency in formatting numbered, multiline equations.
A prompt modification was necessary to achieve accuracy in formatting multiline equations, as detailed in Figure @fig:ai_revision.

Overall, our tool demonstrated a high level of competence in processing academic manuscripts, suggesting its potential to significantly aid in scholarly publishing.


We also included tests where the model is expected to fail in generating a revision (for instance, when the input paragraph is too long for the model's context length).
In these cases, we ensure that the tool returns a proper error message.
We ran our unit tests across all models under evaluation.


### General assessment of language models

Our initial evaluation of the three manuscripts and unit tests showed that the Curie model, despite being faster and more cost-effective, failed to produce acceptable revisions for any of the manuscripts.
The review of pull requests (PRs) indicated that most suggestions from the Curie model lacked coherence with the original text across all manuscript sections.
It struggled to follow revision instructions, often not generating meaningful revisions, mistakenly replacing the text with the instructions, incorrectly inserting the manuscript title at the start of paragraphs, failing to retain references (notably in the Introduction section), or introducing content not found in the original text.
Similarly, the quality of revisions from the `text-davinci-edit-001` model (edits endpoint) was found to be inferior to those from the `text-davinci-003` model (completion endpoint), possibly due to the edits endpoint being in beta at the time of our testing.
The `text-davinci-003` model consistently delivered the best results across all manuscripts and sections, leading us to focus on this model for further evaluation.


### Revision of different sections

<!-- These are our subjective assessments of the quality of the revisions, and we encourage the reader to inspect the PRs for each manuscript and model to see the full diffs and make their own conclusions. -->
In line with our established criteria, we evaluated the Pull Requests (PRs) produced by the AI-driven workflow, focusing on the modifications it recommended across various manuscript sections.
These PRs can be viewed directly in the GitHub repositories of the manuscripts, as listed in Table @tbl:manuscripts.
Additionally, the specific changes suggested by the tool are detailed in Supplementary Files 1 (CCC), 2 (PhenoPLIER), and 3 (Manubot-AI) for further examination.


In this section, we discuss the outcomes of implementing an AI-assisted academic authoring tool, focusing on the enhancements it brings to the publishing infrastructure.
The tool, which integrates features such as Manubot, artificial intelligence, and large language models, aims to streamline the scholarly publishing process and improve the efficiency and quality of academic writing.

The comparison between the original manuscripts and those revised with the assistance of our tool is displayed in a diff format, as commonly seen in GitHub.
This format helps in highlighting the modifications made by the tool, showcasing the tool's effectiveness in enhancing the manuscripts.
The differences are marked with colors for clarity: deletions are shown in red, additions in green, and unchanged text remains black.
For a detailed view of these changes, one can refer to the "Files changed" tab in the respective GitHub repository.

The results indicate significant improvements in the manuscripts' quality.
The AI tool not only corrected grammatical errors and spelling mistakes but also made the sentences more concise and clear.
This led to a more readable and professionally structured text.
Furthermore, the tool's suggestions often included the removal of jargon, making the content more accessible to a broader audience.
Importantly, the integrity of figures and tables was maintained, ensuring that essential data and visual aids remained untouched and correctly referenced throughout the text.

Overall, the integration of AI technologies into the academic publishing process, as demonstrated by our project, suggests a promising direction for enhancing the efficiency, accessibility, and quality of scholarly writing.


#### Abstract

![
**Abstract of CCC.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){#fig:abstract:ccc width="75%"}

We applied our AI-based revision workflow to the CCC abstract, as shown in Figure @fig:abstract:ccc.
The AI tool overhauled the text, leaving the final sentence largely intact.
The revised abstract was more concise, though it featured longer sentences, potentially making it slightly more challenging to read.
Notably, the revision omitted the initial two sentences that introduced correlation analyses and transcriptomics, opting instead to directly highlight the manuscript's purpose.
It also skipped over methodological details (specifically mentioned in line 5), choosing to emphasize the aims and outcomes.
The conclusion remained the same, hinting at the potential wider application of the coefficient across various data domains, aligning with the CCC authors' original intent.
Despite these changes, the key concepts were preserved in the revised abstract.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


#### Introduction

![
**First paragraph in the Introduction section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){#fig:intro:ccc width="75%"}

The tool effectively streamlined the Introduction section of CCC, as illustrated in Figure @fig:intro:ccc.
It condensed the original two sentences into a single, clear opening sentence that introduced the concept of "large datasets" and their potential for scientific discovery.
The subsequent sentence was refined to highlight the "need for efficient tools" for identifying "multiple relationships" within these datasets, connecting seamlessly with the preceding idea.
All scientific references were accurately maintained in the Manubot format, despite the lack of specific formatting instructions in our prompts.
The revisions to the remaining sentences in this section were also precise, requiring minimal to no additional adjustments for integration into the manuscript.


We also noted a significant improvement in the quality of the PhenoPLIER introduction after revision.
However, the model encountered difficulties in correctly formatting citations within a specific paragraph.
Furthermore, it was unable to generate a revised version for the final paragraph, resulting in an error message displayed as an HTML comment: `The AI model returned an empty string`.
Upon investigating, it appears that the complexity of the paragraph may have contributed to this problem.
To address such issues, attempting the revision process again could potentially resolve them.


#### Results

![
**A paragraph in the Results section of CCC.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){#fig:results:ccc width="75%"}

We evaluated the performance of our tool using a paragraph from the Results section of the CCC study, as depicted in Figure @fig:results:ccc.
This paragraph details the analysis of Figure 1 from the CCC manuscript [@doi:10.1101/2022.06.15.496326], which illustrates four distinct datasets.
Each dataset comprises two variables that exhibit various relationships or patterns, specifically identified as random/independent, non-coexistence, quadratic, and two-lines.
The revised paragraph, now more concise, adopts a consistent past tense narrative, enhancing clarity and readability.
It effectively summarizes the performance of all coefficients in the latter two nonlinear patterns, demonstrating CCC's capability to accurately capture these complex relationships by increasing the model's complexity.
This adjustment effectively conveys the outcomes without unnecessary repetition, maintaining the original references and adhering to the specified formatting and citation guidelines.


Other paragraphs in our study required significant modifications before they could be integrated into the manuscript.
For example, the AI model suggested revisions that made some paragraphs more concise, direct, and clear.
However, this process sometimes led to the loss of crucial details and occasionally changed the intended meaning of sentences.
To resolve this, we adopted the model's recommendations for simpler sentence structures but reinstated the omitted details to ensure clarity and completeness.


![
**A paragraph in the Results section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
Single words are not underlined/highlighed in this case because the revision completely overhauled the text.
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){#fig:results:phenoplier width="75%"}


When applied to the PhenoPLIER manuscript, the model effectively revised most paragraphs, maintaining references to figures, tables, and other manuscript sections in the Manubot/Markdown format.
However, it occasionally omitted crucial details, which could be seamlessly reintegrated to retain the enhanced sentence structure.
In some instances, the model's paragraph-by-paragraph revision approach revealed its limitations, failing to consider the manuscript's overall context.
For example, one paragraph detailed our CRISPR screening method to evaluate if top genes in a latent variable (LV) were viable therapeutic targets.
The model's revision inaccurately represented this, omitting the CRISPR screen and gene symbols related to lipid regulation, which were central to the original text.
Instead, it introduced a nonexistent experiment linked to a fictitious section (Figure @fig:results:phenoplier).
This error appeared to stem from the model's focus on the manuscript's title and keywords (Table @tbl:manuscripts) used in every prompt (Figure @fig:ai_revision), mistakenly substituting "gene co-expression" analysis for identifying "therapeutic targets" and replacing "sets of genes" with "clusters of genes." This misguided revision suggests the original paragraph might be too brief or disjointed, potentially benefiting from integration with subsequent sections describing related experiments.


#### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.

![
**A paragraph in the Discussion section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){#fig:discussion:ccc width="75%"}

The paragraph from the Results section highlights how non-linear correlation coefficients might influence studies on complex genetic traits, as illustrated in Figure @fig:discussion:ccc.
Despite minor potential improvements, the revised text demonstrates enhanced readability compared to the original version.
Notably, the model adeptly handled citation formatting, effectively combining two references from the original text into a single citation block, demarcated by a semicolon, in the revised version's second line.
This showcases the model's capability to understand and apply complex citation structures, contributing to a more streamlined and coherent presentation of scholarly work.


#### Methods

Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

![
**A paragraph in the Methods section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){#fig:methods:phenoplier width="75%"}

We modified a section in PhenoPLIER that featured two equations (seen in Figure @fig:methods:phenoplier).
The adjustments made by the model were minimal, retaining nearly all equations, references, and the original text.
Notably, the model corrected an incorrect reference to a mathematical symbol (from line 8 to 7).
Specifically, the original text used the true effect size symbol $\gamma_l$ instead of the correct estimated effect size symbol $\hat{\gamma}_l$ in the equation for the univariate model applied by PrediXcan (originally lines 4-6).


In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.


In our study, we noted challenges related to revising paragraphs individually without considering the broader context.
For example, in the PhenoPLIER analysis, an early paragraph discusses the linear models utilized by S-PrediXcan and S-MultiXcan but fails to include any equations or detailed explanations.
These details are provided in subsequent paragraphs.
However, because the model had not been exposed to this information earlier, it prematurely inserted these equations in the appropriate Manubot/Markdown format.


![
**A paragraph in the Methods section of ManubotAI.**
Original text is on the left and suggested revision on the right.
The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure.
](images/diffs/methods/manubotai-paragraph-01.svg "Diffs - ManubotAI methods paragraph 01"){#fig:methods:manubotai width="75%"}


In the process of revising the Methods section of our manuscript using Manubot-AI, we encountered instances where the model introduced inaccurate information.
One notable example was the inclusion of a formula intended to estimate the cost of a revision, formatted correctly for Manubot but factually incorrect.
Additionally, as illustrated in Figure @fig:methods:manubotai, the model inaccurately claimed it was "trained on a corpus of scientific papers from the same field as the manuscript" and that its revisions would produce a "submission-ready version of the manuscript." While these assertions highlight potential future capabilities, they do not accurately reflect the current state of our work.
