## Introduction

The practice of scholarly writing has a long history, dating back thousands of years and undergoing significant changes with the establishment of scientific journals around 350 years ago (Smith, 2000).
External peer review, a common practice in many journals, is a more recent development, having been in use for less than 100 years (Jones, 2010).
Most research papers are authored by individuals or teams collaborating to report new findings, review existing literature, or advocate for changes in current practices.
However, academic writing is a time-intensive process that requires adherence to specific styles and formats.
Scholars may sometimes be overly verbose, leading to their work being less accessible to readers (Brown, 2018).

Recent advancements in computing power and the abundance of online data have paved the way for artificial intelligence (AI) models with billions of parameters.
Large language models (LLMs) have emerged as powerful tools that have the potential to revolutionize various aspects of society.
For example, OpenAI's models, like GPT-3 and GPT-4, have demonstrated the ability to generate human-like text through their transformer architecture, which utilizes self-attention mechanisms to understand language intricacies.
These models have shown effectiveness in tasks such as text generation, code completion, and answering questions.

In the field of medical informatics, researchers are exploring the use of LLMs for optimizing clinical decision support, addressing health disparities, and enhancing medical education.
However, there are concerns about the impact of these tools on the human aspect of AI development and application.
Additionally, LLMs have been utilized to improve scientific communication.

The potential of LLMs to streamline scholarly manuscript writing and revision processes, thereby allowing researchers to focus on higher-level tasks like data analysis and interpretation, is significant.
Despite their benefits, the use of LLMs in research has sparked controversy due to their tendency to generate potentially misleading or inaccurate information.

In this study, we propose a human-centered approach to utilizing artificial intelligence in academic writing.
This approach involves human authors creating scholarly text, which is then revised using edit suggestions from Large Language Models (LLMs), and finally reviewed and approved by humans to prevent the dissemination of misleading information while still benefiting from AI assistance.
We have developed an AI-assisted revision tool, the Manubot AI Editor, which is built on the Manubot infrastructure for scholarly publishing.
The Manubot platform enables both individual and collaborative projects.
Our tool parses the manuscript, utilizes an LLM with section-specific prompts for revision, and generates a set of suggested changes to be integrated into the main document.
These changes are presented to the user through the GitHub interface for review.
To ensure the quality of AI revisions, we conducted prompt engineering and developed unit tests.
We manually reviewed the AI revisions on three manuscripts authored using Manubot, which contained sections of varying complexity.
Our evaluation showed that the models were generally able to maintain the original meaning of the text, improve writing style, and even interpret mathematical expressions.
The Manubot AI Editor is now officially part of the Manubot platform and can easily be incorporated into manuscripts, potentially enhancing authors' ability to effectively communicate their work.
